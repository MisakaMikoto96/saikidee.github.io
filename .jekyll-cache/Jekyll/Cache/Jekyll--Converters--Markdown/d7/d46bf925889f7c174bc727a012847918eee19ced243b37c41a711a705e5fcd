I"×<p>æ”¶å½•å’Œåˆ†äº«ä¸€äº›è¯­éŸ³åˆæˆé¢†åŸŸè®ºæ–‡é˜…è¯»æ¸…å•, åŒ…æ‹¬å‰ç«¯ã€å£°å­¦æ¨¡å‹ã€å£°ç å™¨ã€VCã€æƒ…æ„Ÿåˆæˆã€é£æ ¼è¿ç§»ã€æ­Œå£°åˆæˆç­‰å†…å®¹ã€‚</p>

<h1 id="frontend">Frontend</h1>
<h2 id="å¤šéŸ³å­—">å¤šéŸ³å­—</h2>

<p><a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2292.pdf">Disambiguation of Chinese Polyphones in an End-to-End Framework with Semantic Features Extracted by Pre-trained BERT</a>ï¼ˆ2019ï¼‰</p>

<p><a href="https://arxiv.org/abs/2004.03136">g2pM: A Neural Grapheme-to-Phoneme Conversion Package for Mandarin Chinese Based on a New Open Benchmark Dataset</a>ï¼ˆ2020ï¼‰</p>

<h2 id="éŸµå¾‹">éŸµå¾‹</h2>

<hr />

<h1 id="vocoder">Vocoder</h1>
<p><strong>WavenNet</strong> <a href="https://arxiv.org/abs/1712.05884.pdf">Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions</a>ï¼ˆ2017ï¼‰</p>

<p><strong>WaveGlow</strong> <a href="https://arxiv.org/abs/1811.00002.pdf">WaveGlow: A Flow-based Generative Network for Speech Synthesis</a>ï¼ˆ2018ï¼‰</p>

<p><strong>WaveRNN</strong> <a href="https://arxiv.org/abs/1802.08435.pdf">Efficient Neural Audio Synthesis</a> ï¼ˆ2018ï¼‰</p>

<p><strong>FastSpeech1</strong> <a href="https://arxiv.org/abs/1905.09263.pdf">FastSpeech: Fast, Robust and Controllable Text to Speech</a> ï¼ˆ2019ï¼‰</p>

<p><strong>FastSpeech2</strong> <a href="https://arxiv.org/pdf/2006.04558v3.pdf">FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</a> (2020)</p>

<p><strong>Mel-Gan</strong> <a href="https://arxiv.org/abs/1910.06711.pdf">MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis</a> (2019)</p>

<p><strong>Multi-band MelGan</strong> <a href="https://arxiv.org/pdf/2005.05106.pdf">Multi-band MelGAN: Faster Waveform Generation for High-Quality Text-to-Speech</a> (2020)</p>

<p><strong>Parallel WaveGan</strong> <a href="https://arxiv.org/abs/1909.01700.pdf">DurIAN: Duration Informed Attention Network For Multimodal Synthesis</a> (2019ï¼‰</p>

<hr />

<h1 id="acoustic-model">Acoustic Model</h1>

<p><strong>Tacotron</strong>ï¼š<a href="https://arxiv.org/pdf/1703.10135.pdf">Tacotron: Towards End-to-End Speech Synthesis</a> ï¼ˆ2017ï¼‰</p>

<p><strong>Tacotron2</strong>ï¼š <a href="https://arxiv.org/pdf/1712.05884.pdf">Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions</a> ï¼ˆ2017ï¼‰</p>

<p><strong>Transformer-TTS</strong>ï¼š<a href="https://arxiv.org/pdf/1809.08895.pdf">Neural Speech Synthesis with Transformer Network</a> ï¼ˆ2018ï¼‰</p>

<p><strong>Durian</strong> <a href="https://arxiv.org/abs/1909.01700.pdf">DurIAN: Duration Informed Attention Network For Multimodal Synthesis</a> (2019ï¼‰</p>

<p><strong>AdaDurain</strong> <a href="https://arxiv.org/pdf/2005.05642.pdf">AdaDurIAN: Few-shot Adaptation for Neural Text-to-Speech with DurIAN</a> (2020)</p>

<hr />

<h1 id="expressive-tts-style-transfer--control--disentangle">Expressive TTS (style transfer &amp; control &amp; disentangle)</h1>
<h2 id="gst-based">GST-based</h2>
<p><a href="https://arxiv.org/abs/1909.01700.pdf">DurIAN: Duration Informed Attention Network For Multimodal Synthesis</a> (2019ï¼‰</p>

<p><a href="https://arxiv.org/abs/1904.02373.pdf">Multi-reference Tacotron by Intercross Training for Style Disentangling,Transfer and Control in Speech Synthesis</a>(2019)</p>

<p><a href="https://arxiv.org/abs/1803.09017.pdf">Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis</a>(2018)</p>

<h2 id="vae-based">VAE-based</h2>
<p><a href="https://arxiv.org/abs/2002.03785.pdf">Fully-hierarchical fine-grained prosody modeling for interpretable speech synthesis</a>(2020)</p>

<p><a href="https://arxiv.org/abs/1906.03402.pdf">Effective Use of Variational Embedding Capacity in Expressive End-to-End Speech Synthesis</a>(2019)</p>

<p><a href="https://arxiv.org/abs/1811.02122.pdf">Robust and fine-grained prosody control of end-to-end speech synthesis</a>(2018)</p>

<p><a href="https://arxiv.org/abs/1810.07217.pdf">Hierarchical Generative Modeling for Controllable Speech Synthesis</a>(2018)</p>

<p><a href="https://arxiv.org/abs/1812.04342.pdf">Learning latent representations for style control and transfer in end-to-end speech synthesis</a>(2018)</p>

<h3 id="advanced">Advanced</h3>
<p><a href="https://arxiv.org/abs/1905.04982.pdf">Learning Hierarchical Priors in VAEs</a>ï¼ˆ2019ï¼‰</p>

<p><a href="https://arxiv.org/abs/1804.03599.pdf">Understanding disentangling in Î²-VAE</a> (2018)</p>

<p><a href="https://arxiv.org/abs/1810.00597.pdf">Taming vae</a> (2018)</p>

<p><a href="https://openreview.net/pdf?id=Sy2fzU9gl">beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</a> (2017)</p>

<p><a href="http://proceedings.mlr.press/v84/tomczak18a/tomczak18a.pdf">VAE with a VampPrior</a> (2017)</p>

<p><a href="https://arxiv.org/abs/1711.00464.pdf">Fixing a Broken ELBO</a>(2017)</p>

<p><a href="https://arxiv.org/abs/1606.04934.pdf">Improving Variational Inference with Inverse Autoregressive Flow </a> (2016)</p>

<p><a href="https://arxiv.org/abs/1509.00519.pdf">Importance weighted autoencoders</a>ï¼ˆ2015ï¼‰</p>

<hr />

<h1 id="sing-synthesis">Sing Synthesis</h1>

<hr />

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>
:ET